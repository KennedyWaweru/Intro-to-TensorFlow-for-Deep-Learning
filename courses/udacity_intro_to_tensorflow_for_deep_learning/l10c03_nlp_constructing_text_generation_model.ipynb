{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "91667f93-05e7-45fa-a627-17509c8d5fbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-11 10:19:32--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.125.113, 142.250.125.102, 142.250.125.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.125.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/61po5edv4ed2ufdl691dvvqlpfg7ri63/1652264325000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-11 10:19:34--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/61po5edv4ed2ufdl691dvvqlpfg7ri63/1652264325000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M   101MB/s    in 0.7s    \n",
            "\n",
            "2022-05-11 10:19:35 (101 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "4c080dcc-49a8-4fc9-ae98-594eafc780d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "9286fe97-154f-4c54-f164-247c29f59e8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "bc0c6666-7c51-4172-8852-fbd4d556e5b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 9s 13ms/step - loss: 5.9900 - accuracy: 0.0247\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.4383 - accuracy: 0.0343\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.3733 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.3218 - accuracy: 0.0399\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.2482 - accuracy: 0.0399\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.1753 - accuracy: 0.0494\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.1103 - accuracy: 0.0520\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.0546 - accuracy: 0.0399\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.9963 - accuracy: 0.0641\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.9353 - accuracy: 0.0676\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.8687 - accuracy: 0.0792\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.7866 - accuracy: 0.0797\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.6954 - accuracy: 0.0908\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.6038 - accuracy: 0.0964\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.5113 - accuracy: 0.1080\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.4264 - accuracy: 0.1095\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.3358 - accuracy: 0.1150\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.2576 - accuracy: 0.1332\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.1707 - accuracy: 0.1569\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.0857 - accuracy: 0.1635\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.9990 - accuracy: 0.1852\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.9211 - accuracy: 0.1882\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.8468 - accuracy: 0.2099\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.7673 - accuracy: 0.2225\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.7080 - accuracy: 0.2275\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.6242 - accuracy: 0.2386\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.5539 - accuracy: 0.2437\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.4965 - accuracy: 0.2619\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.4310 - accuracy: 0.2765\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.3631 - accuracy: 0.2941\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.3063 - accuracy: 0.3118\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.2432 - accuracy: 0.3259\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.1858 - accuracy: 0.3446\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.1165 - accuracy: 0.3557\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.0646 - accuracy: 0.3643\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.9913 - accuracy: 0.3789\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.9372 - accuracy: 0.4001\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.8760 - accuracy: 0.4168\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.8292 - accuracy: 0.4178\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.7702 - accuracy: 0.4369\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.7286 - accuracy: 0.4470\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.6759 - accuracy: 0.4556\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.6429 - accuracy: 0.4627\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.5886 - accuracy: 0.4697\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.5453 - accuracy: 0.4768\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.4808 - accuracy: 0.4970\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.4483 - accuracy: 0.4980\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.4034 - accuracy: 0.5121\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.3533 - accuracy: 0.5116\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.3073 - accuracy: 0.5277\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.2890 - accuracy: 0.5227\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.2378 - accuracy: 0.5328\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.1904 - accuracy: 0.5474\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.1586 - accuracy: 0.5520\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.1239 - accuracy: 0.5600\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.0765 - accuracy: 0.5721\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.0423 - accuracy: 0.5792\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.0112 - accuracy: 0.5883\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.9754 - accuracy: 0.5923\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.9417 - accuracy: 0.5928\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8994 - accuracy: 0.6090\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8715 - accuracy: 0.6206\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8256 - accuracy: 0.6271\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.7987 - accuracy: 0.6403\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.7624 - accuracy: 0.6443\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.7314 - accuracy: 0.6609\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.7133 - accuracy: 0.6549\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.6758 - accuracy: 0.6675\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.6540 - accuracy: 0.6665\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.6181 - accuracy: 0.6781\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5817 - accuracy: 0.6816\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5737 - accuracy: 0.6771\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5651 - accuracy: 0.6862\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5467 - accuracy: 0.6811\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5046 - accuracy: 0.6948\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.4800 - accuracy: 0.6968\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4560 - accuracy: 0.7074\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4201 - accuracy: 0.7119\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4081 - accuracy: 0.7079\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.3848 - accuracy: 0.7139\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.3544 - accuracy: 0.7230\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.3237 - accuracy: 0.7265\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2969 - accuracy: 0.7311\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2832 - accuracy: 0.7402\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2777 - accuracy: 0.7336\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2580 - accuracy: 0.7417\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2965 - accuracy: 0.7301\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2689 - accuracy: 0.7386\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2213 - accuracy: 0.7538\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1859 - accuracy: 0.7543\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1628 - accuracy: 0.7679\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1394 - accuracy: 0.7684\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1142 - accuracy: 0.7725\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1124 - accuracy: 0.7770\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0924 - accuracy: 0.7780\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0698 - accuracy: 0.7800\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0508 - accuracy: 0.7846\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0390 - accuracy: 0.7896\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0251 - accuracy: 0.7911\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0176 - accuracy: 0.7926\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0015 - accuracy: 0.7921\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9847 - accuracy: 0.7962\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9636 - accuracy: 0.8083\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9508 - accuracy: 0.8063\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9488 - accuracy: 0.8063\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9370 - accuracy: 0.8032\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9183 - accuracy: 0.8068\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9286 - accuracy: 0.8083\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9038 - accuracy: 0.8153\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8802 - accuracy: 0.8194\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8640 - accuracy: 0.8204\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8489 - accuracy: 0.8204\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8455 - accuracy: 0.8264\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8384 - accuracy: 0.8199\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8379 - accuracy: 0.8204\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8216 - accuracy: 0.8280\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8242 - accuracy: 0.8249\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7993 - accuracy: 0.8280\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7904 - accuracy: 0.8310\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7832 - accuracy: 0.8385\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8233 - accuracy: 0.8189\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7825 - accuracy: 0.8274\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7589 - accuracy: 0.8340\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7403 - accuracy: 0.8421\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7244 - accuracy: 0.8446\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7165 - accuracy: 0.8431\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7060 - accuracy: 0.8476\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6973 - accuracy: 0.8486\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7028 - accuracy: 0.8446\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6990 - accuracy: 0.8431\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6952 - accuracy: 0.8426\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6741 - accuracy: 0.8461\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6659 - accuracy: 0.8522\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6654 - accuracy: 0.8486\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6576 - accuracy: 0.8527\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6517 - accuracy: 0.8547\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6661 - accuracy: 0.8491\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6490 - accuracy: 0.8537\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6386 - accuracy: 0.8557\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6398 - accuracy: 0.8572\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6271 - accuracy: 0.8577\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6110 - accuracy: 0.8628\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5992 - accuracy: 0.8633\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5890 - accuracy: 0.8618\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5903 - accuracy: 0.8643\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6007 - accuracy: 0.8633\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5786 - accuracy: 0.8658\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5654 - accuracy: 0.8693\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5586 - accuracy: 0.8698\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5505 - accuracy: 0.8708\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5438 - accuracy: 0.8729\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5383 - accuracy: 0.8754\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5332 - accuracy: 0.8779\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5300 - accuracy: 0.8724\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5207 - accuracy: 0.8784\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5146 - accuracy: 0.8794\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5110 - accuracy: 0.8749\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5086 - accuracy: 0.8734\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5257 - accuracy: 0.8779\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5170 - accuracy: 0.8718\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5099 - accuracy: 0.8769\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5046 - accuracy: 0.8769\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5063 - accuracy: 0.8724\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4962 - accuracy: 0.8764\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4917 - accuracy: 0.8840\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4944 - accuracy: 0.8739\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4841 - accuracy: 0.8819\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4902 - accuracy: 0.8809\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4839 - accuracy: 0.8759\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4673 - accuracy: 0.8829\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4528 - accuracy: 0.8885\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4450 - accuracy: 0.8895\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4416 - accuracy: 0.8875\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4393 - accuracy: 0.8875\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.4300 - accuracy: 0.8890\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4232 - accuracy: 0.8900\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4185 - accuracy: 0.8961\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4218 - accuracy: 0.8930\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4345 - accuracy: 0.8860\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4214 - accuracy: 0.8910\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4103 - accuracy: 0.8956\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4032 - accuracy: 0.8981\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4009 - accuracy: 0.8946\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3964 - accuracy: 0.8991\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3960 - accuracy: 0.8956\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3963 - accuracy: 0.8961\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3932 - accuracy: 0.8966\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3925 - accuracy: 0.8966\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3984 - accuracy: 0.8971\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3918 - accuracy: 0.8961\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3820 - accuracy: 0.8956\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4008 - accuracy: 0.8925\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3800 - accuracy: 0.8961\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3735 - accuracy: 0.8996\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3732 - accuracy: 0.8966\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3690 - accuracy: 0.8951\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3649 - accuracy: 0.8986\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3601 - accuracy: 0.8966\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3537 - accuracy: 0.9011\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3526 - accuracy: 0.9001\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "f05690a3-bf10-435a-f384-7a79902a87cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "282614f9-e1db-4cb1-be74-8d75adb07e22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "roses are red gone for my lot of every little touch ive wait wait talk would touch sucker crazy crazy crazy crazy would touch night night night heartaches blue life do life boomaboomerang boomaboomerang is new see had never would life over take over over you humdehumhum cassandra over you humdehumhum cassandra always would touch before before feather past cassandra over you humdehumhum together together life have you humdehumhum life cassandra cassandra i didnt kind but at sunrise about on and guys talk do what on together harbor at tomorrow even better for too hard to wait to even touch night feather even\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"roses are red\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AGIPRlGasZz2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}